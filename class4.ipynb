{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35b6b0e",
   "metadata": {},
   "source": [
    "# Scientific Programming: A Crash Course\n",
    "\n",
    "## Class 4 – The Scientific Stack – 24 Feb 2022\n",
    "\n",
    "Today we're going to start doing the *science* part of scientific programming. By the end of this session, I hope you will be able to load a dataset and make some basic plots. To do this, we are going to use three packages that provide lots of functionality well beyond what you can easily achieve in Python alone. These packages are part of what is know as the \"scientific stack\" (i.e. the stack of packages that are widely used in science). The scientific stack is not strictly defined, but most people would consider the following three packages (at least) to be core members:\n",
    "\n",
    "- [NumPy](https://numpy.org) - Arrays and numerical computation\n",
    "- [Pandas](https://pandas.pydata.org) – Data frames\n",
    "- [Matplotlib](https://matplotlib.org) – Plotting and visualization\n",
    "\n",
    "However, here are some other packages that could also be considered part of the stack:\n",
    "\n",
    "- [NetworkX](https://networkx.org) – Graphs and network analysis\n",
    "- [Numba](http://numba.pydata.org) – Just-in-time compiler (make things faster)\n",
    "- [PyMC](https://docs.pymc.io) and [Bambi](https://bambinos.github.io/bambi/main/) - Bayesian statistical modelling and MCMC sampling\n",
    "- [Scikit-Learn](https://scikit-learn.org/stable/index.html) - Machine learning tools\n",
    "- [Scikit-Optimize](https://scikit-optimize.github.io/stable/) – Black box function optimization\n",
    "- [SciPy](https://scipy.org) – Linear algebra, optimization, signal processing\n",
    "- [StatsModels](https://www.statsmodels.org/stable/index.html) – Traditional frequentist stats, including regression models\n",
    "- [SymPy](https://www.sympy.org/en/index.html) - Mathematics and symbolic computation\n",
    "- [TensorFlow](https://www.tensorflow.org) and [PyTorch](https://pytorch.org) – Neural networks\n",
    "- [Xarray](https://docs.xarray.dev/en/stable/) - Efficient high-dimensional array handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3114aa7",
   "metadata": {},
   "source": [
    "## Installing packages\n",
    "\n",
    "The ordinary way to install Python packages is to use the `pip` tool. This downloads a package from the [Python Package Index (PyPI)](https://pypi.org) and installs it. It also automatically downloads and installs any other packages that your chosen package depends on – its \"dependencies.\" For example, `package_a` might depend on `package_b`, which depends on `package_c` and `package_d`. Using `pip` to install `package_a` will install all four of the packages. Here are a few useful commands (note, that these need to be used at the terminal, not in the Python interpreter or in a notebook):\n",
    "\n",
    "- `pip list` – list the packages you have installed\n",
    "- `pip show package_name` – get info about a particular package you have installed\n",
    "- `pip install package_name` – install a new package and its dependencies\n",
    "- `pip uninstall package_name` – uninstall a package (this does *not* remove its dependencies)\n",
    "\n",
    "However, installing packages can also get a little more complicated depending on how you set things up. If you are using Anaconda, for example, then it's best to use their own tool `conda` to install packages. This works similarly to `pip` but also provides more options to create separate **virtual environments**. A virtual environment is an isolated space in which you install a collection of packages. This allows you to maintain separate environments for separate projects. For example, in one project, you might be using version 1 of `package_x` and in another project you might be using version 2 of `package_x`. By maintaining separate environments, you can avoid potential conflicts. I don't want to go into more detail today about virtual environments, but they are extremely useful, and if you are planning to use Python in the long term, I would highly recommend that you start using them.\n",
    "\n",
    "Depending on how things are set up on your computer, you may already have the right packages installed. Try running the following line of code to see if `numpy` is already installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c398edb",
   "metadata": {},
   "source": [
    "If it fails to import, you will first need to install it. Either by using `conda` (if you are using the Anaconda distribution):\n",
    "\n",
    "```bash\n",
    "conda install numpy\n",
    "```\n",
    "\n",
    "Or `pip` if you are using a vanilla version of Python:\n",
    "\n",
    "```bash\n",
    "pip install numpy\n",
    "```\n",
    "\n",
    "Packages can also be managed and installed using Anaconda's GUI interface, although I haven't used this myself, so I'm not exactly sure how it works. Check out this link for more help: https://docs.anaconda.com/anaconda/navigator/getting-started/#navigator-managing-packages\n",
    "\n",
    "Check that you also have Pandas and Matplotlib installed too – we'll also be using those today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ca8ce",
   "metadata": {},
   "source": [
    "## NumPy\n",
    "\n",
    "NumPy (officially pronounced /nʌmpaɪ/ although many people call it /nʌmpi/) is *the* most fundamental package in the scientific stack. Whenever I need to do something mathy or sciency, NumPy is always the very first thing I import, so let's go ahead and do that right now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca27afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae78aac",
   "metadata": {},
   "source": [
    "Note that I am importing `numpy` as `np` here. This means I am importing the package but giving it a different, shorter name, so that I don't have to keep typing `numpy` all the time. This convention of importing `numpy` as `np` is very common, so it's good to get into the habit.\n",
    "\n",
    "### Arrays\n",
    "\n",
    "Right, so what is NumPy anyway? NumPy provides support for array-based numerical computation, something that is not directly available in the core Python language. The closest thing in base-Python is the `list` data type, but NumPy arrays extend the `list` concept much further. If you are coming from the Matlab world, you should find NumPy arrays very familiar. There are also a lot of similarities with vectors in R.\n",
    "\n",
    "Let's make a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab83feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array = np.array( [1, 2, 3, 4, 5] )\n",
    "print(my_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bbf449",
   "metadata": {},
   "source": [
    "As you can see, an array basically looks like a list. In fact, to create this array I literally made a list first (recall that square brackets are used to make lists) and then I passed this list into the `np.array()` function to convert it into an array.\n",
    "\n",
    "So, what can I do with this array? Well, first, look what happens if I perform some basic mathematical operations with the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479fd6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_array + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc529e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_array * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6531d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_array / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a62ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_array - 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8684274",
   "metadata": {},
   "source": [
    "As you can see, when you perform some mathematical operation with a scalar (a single number), that operation is applied to all the numbers in the array simultaneously. If you wanted to achieve the same effect with base-Python, you'd have to do something a bit more awkward, perhaps involving a list comprehension like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [1, 2, 3, 4, 5]\n",
    "print([x*10 for x in my_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f245426",
   "metadata": {},
   "source": [
    "You can also perform mathematical operations with two (or more) arrays, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = np.array([1, 2, 3, 4, 5])\n",
    "array2 = np.array([1, 10, 100, 1000, 1000])\n",
    "\n",
    "print(array1 * array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac0596",
   "metadata": {},
   "source": [
    "Do you see what happened? Each number in `array1` was multiplied by the corresponding number in `array2`. This is called **elementwise multiplication**. As you would expect, you can also do elementwise addition, subtraction, division, and exponentiation. The key thing to remember is that the arrays need to have the same length so that they can be [\"broadcast\"](https://www.tutorialspoint.com/numpy/numpy_broadcasting.htm) together. Try writing some more examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186fc94c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4473e0aa",
   "metadata": {},
   "source": [
    "Arrays can have more than one dimension. Here's a two-dimensional array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961bf7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_2d = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "print(array_2d * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08522086",
   "metadata": {},
   "source": [
    "You don't need to create the array with indentation, as I have done here – I just did that to make the code more readable. A 2D array is like a list of lists, where the inner lists are all of the same length.\n",
    "\n",
    "Now let's multiply a 2D array and 1D array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_1d = np.array([10, 100, 1000])\n",
    "\n",
    "array_2d = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "print(array_1d * array_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1853a54c",
   "metadata": {},
   "source": [
    "Do you see what happened? The elementwise multiplication is applied to each row. How would you do the same to each column – elementwise multiplication across the columns (hint: it's not very obvious, but you need a column array).\n",
    "\n",
    "Arrays have several attributes that are sometimes useful to access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa106846",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(array_2d.ndim) # Number of dimensions\n",
    "print(array_2d.shape) # Shape of the dimensions (n rows, n columns)\n",
    "print(array_2d.size) # Total number of elements / cells\n",
    "print(array_2d.dtype) # Data type of the array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dfb71c",
   "metadata": {},
   "source": [
    "So, in this case, `array_2d` is a two-dimensional array with a 3×3 shape, and it contains nine 64-bit integers. One important thing to note about arrays is that all the elements need to be of the same data type; unlike regular Python lists, you cannot mix different types together. In this case, for example, all the values are integers. This allows NumPy to store and process the numbers very efficiently, which can be important in computationally intensive applications.\n",
    "\n",
    "Unlike regular Python, NumPy gives you much more flexibility over the data type. For example, if you needed to, you could create an array that uses 8-bit integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc724bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([1, 2, 3, 4, 5], dtype=np.int8)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d2e075",
   "metadata": {},
   "source": [
    "8-bit integers allow you to represent $2^8 = 256$ different values, specifically, the numbers -128 through +127. (Try putting some larger values in the array above; what happens? Do you understand why? This is called overflow.) In many cases, 8-bit integers will be very limiting – usually we need to represent a much larger range of numbers. However, if you are doing some kind of computation where you only need small numbers, using 8-bit integers can be beneficial because they use less memory compared to the default 64-bit integers. Base-Python doesn't give you this level of control, but NumPy does.\n",
    "\n",
    "One example of where 8-bit integers are frequently used is in storing RGB (red, green, blue) color data – for example, each pixel in an image is stored as an RGB value. By using 8-bit integers, we can represent 256 shades of red, 256 shades of green, and 256 shades of blue. This means that $256^3 = 16$ million colors can be represented, with each pixel requiring just $8 \\times 3 = 24$ bits of data. If we were using 64-bit integers, each pixel would require $64 \\times 3 = 192$ bits, even if we limited ourselves to making the same 16 million color distinctions.\n",
    "\n",
    "In general, however, you don't need to worry about more exotic data types like 8-bit integers – the default ints and floats will be fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e7fb8",
   "metadata": {},
   "source": [
    "### Array creation\n",
    "\n",
    "Rather than manually creating arrays by passing in a list, as we have been doing so far, there is often a function for creating the particular kind of array you need. For example, if you need an array of consecutive numbers, you can use `np.arange()` (note that this is array-range, the NumPy equivalent of `range()`, not the word *arrange* – for years I misread this function name as *arrange*!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = np.arange(0, 10)\n",
    "print(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a2545",
   "metadata": {},
   "source": [
    "Just like (almost) everything else in Python, ranges are inclusive:exclusive in NumPy. Which reminds me! NumPy has its own set of random functions within the `numpy.random` module. To create an array of 100 random ints, we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac2264",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_nums = np.random.randint(0, 10, 100)\n",
    "print(rand_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8807ec6",
   "metadata": {},
   "source": [
    "Notice how this is different from Python's built-in `random.randint()`, which is weirdly inclusive:inclusive. The NumPy version of the `randint()` function is sensible; all the random integers generated here will be less than 10.\n",
    "\n",
    "Another common type of array you will need is an array of zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a826bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_array = np.zeros( (4, 3) )\n",
    "print(zeros_array)\n",
    "print(zeros_array.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c495b",
   "metadata": {},
   "source": [
    "When you use the `np.zeros()` function to create an array, you have to specify the desired shape; here, I asked for an array of zeros with the shape 4×3. Since I didn't specify a data type, it defaulted to 64-bit floats. Another quick thing to note here: NumPy is [row-major rather than column-major](https://en.wikipedia.org/wiki/Row-_and_column-major_order). This means that the rows are considered to be the first dimension and the columns are the second dimension. Hence, when I asked for a 4×3 array, I got an array with four rows and three columns. The opposite is the case in other scientific programming languages, including Julia, Matlab, and R.\n",
    "\n",
    "You might be wondering why you would ever want an array of zeros. Typically this is useful if you need to do some kind of counting and you need to initialize the counts at zero. For example, imagine you wanted to construct a simple neural network and you needed to initialize the weights to zero.\n",
    "\n",
    "There is also `np.ones()` for creating an array of ones, and `np.full()` for creating an array filled with a particular arbitrary number. Try using them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11382426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02df15a1",
   "metadata": {},
   "source": [
    "Another very common kind of array you will want to create is a linearly-spaced sequence or \"linspace\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c902901",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = np.linspace(0, 1, 21)\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2fc266",
   "metadata": {},
   "source": [
    "So, here, I am creating an array of `21` evenly spaced numbers from `0` to `1`. This is often useful for generating the *x*-values of a graph, and we'll use it for this purpose later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba919340",
   "metadata": {},
   "source": [
    "### Array indexing\n",
    "\n",
    "Indexing an array is very similar to indexing a Python list. To demonstrate, I will  first create an array of the numbers 1–9 reshaped into a 3×3 2D array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cba0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_of_nine = np.arange(1, 10).reshape((3, 3))\n",
    "print(box_of_nine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c32e02",
   "metadata": {},
   "source": [
    "Indexing is done using square brackets, just like with regular Python lists, except now you can also index into multiple dimensions by separating each dimension with a comma. For example, say I wanted to access the value on the first row (index 0 – remember, counting is from zero) and the second column (index 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe2325",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(box_of_nine[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6e788",
   "metadata": {},
   "source": [
    "You can think of the indices like coordinates. Just remember: since NumPy is row-major, the row index is always specified first. Check that you can access all nine numbers by specifying the appropriate row and column indices.\n",
    "\n",
    "NumPy also permits slices, just like regular Python indexing, except the slicing is... you guessed it!... generalized to multiple dimensions. For example, say I wanted to extract the first two rows and all three columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(box_of_nine[0:2, 0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed876116",
   "metadata": {},
   "source": [
    "This can be read as: extract row 0 up-to-but-not-including row 2, and column 0 up-to-but-not-including column 3. Play around with the slices to check you understand. Note also that if you leave one side of the slice blank, the start or end of the dimension is implied. For example, here I extract the middle column (all rows, column 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc3c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(box_of_nine[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7a47e",
   "metadata": {},
   "source": [
    "Like Python lists, arrays are mutable. This means they can be updated with new values. Let's change the middle number to a zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a95f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_of_nine[1, 1] = 0\n",
    "print(box_of_nine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe5f97",
   "metadata": {},
   "source": [
    "You can also modify an entire row or column with a single line of code. Here I will change the middle column to a five:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac85b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_of_nine[:, 1] = 5\n",
    "print(box_of_nine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2cc0e",
   "metadata": {},
   "source": [
    "And now I will increment the middle row by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776157dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_of_nine[1, :] += 1\n",
    "print(box_of_nine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d029e",
   "metadata": {},
   "source": [
    "### NumPy functions\n",
    "\n",
    "NumPy includes lots of common mathematical functions that are designed to work with arrays. For example, you can use `np.sum()` to sum an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_of_nine = np.arange(1, 10).reshape((3, 3))\n",
    "\n",
    "print(box_of_nine)\n",
    "print( np.sum(box_of_nine) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d60f9",
   "metadata": {},
   "source": [
    "`np.sum()` is an example of an aggregator function – a function that reduces a set of values down to a single value. Other examples would be `np.min()`, `np.max()`, and `np.mean()`. These allow you to apply the function along particular dimensions (or axes, as they are referred to). For example, we can sum along the 0th axis (i.e. the rows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d64ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( np.sum(box_of_nine, axis=0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9f0e2",
   "metadata": {},
   "source": [
    "or we can sum along the 1st axis (i.e. the columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2697ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( np.sum(box_of_nine, axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aef885",
   "metadata": {},
   "source": [
    "Try this out with other aggregator functions such as `np.min()` and `np.max()`. Make sure you try to predict what the result will be before running the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b3493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ff389e6",
   "metadata": {},
   "source": [
    "NumPy contains functions for all the common mathematical operations, including:\n",
    "\n",
    "- `np.abs()` – absolute value\n",
    "- `np.argmax()` – argmax\n",
    "- `np.argmin()` – argmin\n",
    "- `np.cos()` – cosine\n",
    "- `np.dot()` – dot product\n",
    "- `np.log()` – natural logarithm\n",
    "- `np.log10()` – base-10 logarithm\n",
    "- `np.log2()` – base-2 logarithm\n",
    "- `np.mean()` – mean\n",
    "- `np.median()` – median\n",
    "- `np.prod()` – product\n",
    "- `np.sin()` – sine\n",
    "- `np.sqrt()` – square root\n",
    "- `np.std()` – standard deviation\n",
    "- `np.tan()` – tangent\n",
    "- `np.var()` – variance\n",
    "\n",
    "and much much more... Many of these functions are also implemented as array methods as well, which are sometimes more convenient to use.\n",
    "\n",
    "Lastly, although I only showed examples of 1D and 2D arrays above, everything we've covered generalizes to *n* dimensions! If you're curious, go back through some of the examples and try playing with higher-dimensional arrays.\n",
    "\n",
    "If you want to learn more about NumPy, check out the user guide: https://numpy.org/doc/stable/user/ The two main reasons why you would want to use NumPy are (1) if you are doing something computationally intensive where efficiency matters (e.g. imaging data or neural nets), and (2) if you are doing something mathy that involves manipulation of vectors and matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca06ae0",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "We briefly encountered Pandas in the last class when we were loading CSV files. Pandas is a package for representing data frames – a concept that will be very familiar to you if you are coming from the R world.\n",
    "\n",
    "What is a data frame? A data frame is essentially a table or spreadsheet. It shares some similarities with 2D arrays, like those we've just been working with above; however, data frames are geared towards data analysis rather than numerical computations. Unlike arrays, data frames have headers, and different data types (ints, strings, Booleans) can be freely mixed together.\n",
    "\n",
    "Let's create a data frame from the `example_data.csv` that we used in the last class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('example_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29270a4",
   "metadata": {},
   "source": [
    "First, similar to NumPy, the conventional way to import Pandas is `as pd`. Again, this is so that we don't have to keep typing `pandas` all the time. I then used the `pd.read_csv()` function to load the CSV file into a data frame. Let's have a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1af30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae50a2",
   "metadata": {},
   "source": [
    "As you can see, this gives us a nice overall summary of the dataset. There are four columns and 15,360 rows. There is also an extra \"column\" on the left which numbers the rows from 0 to 15359, and an extra \"row\" at the top which labels the columns – but these are not true rows and columns, just headers that describe the main content of the data frame. Notice also that we only see the first five rows and the last five rows. If we printed the whole data frame it would be massive. Looking at just the first and last rows is often pretty informative. For example, here we can see that there appear to be 240 subjects.\n",
    "\n",
    "The principle of \"tidy data\" that I mentioned in the last class says that each column should represent a \"variable\" (not in the coding sense, but in the statistical sense), and that each row should represent an observation. It is reasonable to assume, then, that each of the rows here represents a single trial, and that the participant was either correct or incorrect on each trial (represented as `0` or `1`). We also get the sense that there are some different conditions – different test types and category systems. Let's find out what values are present in these two columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd752b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( df['test_type'].unique() )\n",
    "print( df['category_system'].unique() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a159b",
   "metadata": {},
   "source": [
    "Here, the brackets allow us to index each of the columns (first the `test_type` column and then the `category_system` column), and then calling the `.unique()` method show us the unique values that exist in those columns. So, now we can see that this experiment appears to have a 2×3 design – there are two test types and three category systems. Another question we might have is, is this a between-subject or within-subject design? To answer that, we could isolate the data for just one of the participants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe25fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('subject==1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327dd712",
   "metadata": {},
   "source": [
    "Note that this effectively creates a new data frame with only the data from subject 1 (technically, I don't think this creates a new data frame completely; rather, we get a \"view\" into a portion of the original data frame). Based on this, it looks like the experiment is fully between-subject: An individual subject does one of the two test types (this participant did `production`) and one of the three category systems (this participant did `size`). We also see here that the participant did 64 trials.\n",
    "\n",
    "One quick thing to note about the syntax here. `'subject==1'` is a string. This looks a bit awkward, but imagine you instead wrote `subject == 1` (the variable `subject` is `1`) or `'subject' == 1` (the string `'subject'` is 1). These conditions would evaluate as `False` and the query would not be interpretable. When you use the `.query()` method, you express the query as a string and Pandas then parses and interprets that string as some conditional statement pertaining to the data frame. This is different from R, which seems to magically figure out whether something is a variable or a data frame header (I never understood how this works – if someone could enlighten me, I would be grateful!).\n",
    "\n",
    "The next thing we might wonder is how accurate this participant was. First, let's assign the participant's subset of the data frame to a variable for convenient access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b2354",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject1 = df.query('subject==1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d9460",
   "metadata": {},
   "source": [
    "and then we can simply sum the `correct` column to see how many of the trials (out of 64) were correct (the sum of all the zeros and ones effectively just counts the number of ones):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b7f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject1['correct'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee5fe1",
   "metadata": {},
   "source": [
    "Let's express that as a proportion instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a871e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject1['correct'].sum() / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d4810",
   "metadata": {},
   "source": [
    "An alternative way to get this answer would be to take the mean of the column. Taking the mean of a bunch of zeros and ones is mathematically equivalent to calculating the proportion of trials that were correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac72cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject1['correct'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d88167f",
   "metadata": {},
   "source": [
    "Great, so this participant is 86% accurate. Is that good or bad? To answer this we need to look at their performance against all the other participants. It will be easier to do that with some visualizations, so, at this point, we should turn out attention to Matplotlib.\n",
    "\n",
    "But before we move on, if you want to learn more about Pandas, a good place to start is the user guide: https://pandas.pydata.org/docs/user_guide/ There's also a very handy cheat sheet here: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf If you're like me and you can't remember any of the commands, print out the cheat sheet and stick it on the wall behind your computer screen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f65fdc8",
   "metadata": {},
   "source": [
    "## Matplotlib\n",
    "\n",
    "Matplotlib is the most widely-used plotting library in Python. If you're coming from the R world, it is roughly equivalent to ggplot. Matplotlib allows you to create all sorts of plots: the obvious ones, like bar plots, scatter plots, and line plots, but also the less obvious ones like violin plots, heatmaps, and timelines.\n",
    "\n",
    "Let's jump right in and import Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da903c78",
   "metadata": {},
   "source": [
    "Like NumPy and Pandas, this is the conventional way that Matplotlib is imported. Matplotlib includes other modules for dealing with lower-level stuff like shapes and text, but 95% of the time we just need the `pyplot` module, so the convention is to import just that module and call it `plt`.\n",
    "\n",
    "Before we start plotting the data from the previous section, let's look first at some more basic examples to get the general idea. First, let's start with a simple line plot. This type of plot is so basic, Matplotlib literally just calls the relevant function `plot()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81454038",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, 101)\n",
    "y = np.arange(1, 101)\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c27de4",
   "metadata": {},
   "source": [
    "Here I created two sets of values, `x` and `y`, both of which are just the numbers one through one hundred. I then plotted `x` against `y`. To make this fake data look a little more interesting, let's add some random noise to the `y` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb7d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, 101)\n",
    "y = np.arange(1, 101)\n",
    "y += np.random.randint(0, 5, 100)\n",
    "\n",
    "_ = plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff6ab0",
   "metadata": {},
   "source": [
    "(Note here that, on the last line, I assigned the output of the plot to the dummy variable `_`. Normally you don't need to do this, but I'm doing it here because otherwise Jupyter Notebook will print lots of junky output that we don't need to see, like `[<matplotlib.lines.Line2D at 0x151a3dba0>]`. Look back at the previous plot to see what I mean. Assigning the output of the plotting function to a variable just gets it out of the way so that we have a cleaner notebook.)\n",
    "\n",
    "If you're a little unsure what's happening here in terms of the data generation, try taking it step by step (what do `x` and `y` look like, how are we generating and adding the noise?)\n",
    "\n",
    "To make this plot a little more informative, we should add some *x*- and *y*-axis labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Magical X numbers')\n",
    "plt.ylabel('Special Y numbers')\n",
    "_ = plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed093e",
   "metadata": {},
   "source": [
    "And... that shade of blue is pretty boring..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Magical X numbers')\n",
    "plt.ylabel('Special Y numbers')\n",
    "_ = plt.plot(x, y, color='hotpink')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299b6ab3",
   "metadata": {},
   "source": [
    "Another way we might want to plot this data is with a scatter plot. A scatter plot is kinda like a line plot, except the points are not joined together with lines. This usually makes sense when the points are not intrinsically ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f6ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Magical X numbers')\n",
    "plt.ylabel('Special Y numbers')\n",
    "_ = plt.scatter(x, y, color='hotpink')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df87f8",
   "metadata": {},
   "source": [
    "Let's make another scatter plot that looks a bit more realistic – we'll make the noise normally distributed. To do that, we'll use the `np.random.normal()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ad962",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 100)\n",
    "y = x * 2 + np.random.normal(0, 0.2, 100)\n",
    "\n",
    "plt.xlabel('Magical X numbers')\n",
    "plt.ylabel('Special Y numbers')\n",
    "_ = plt.scatter(x, y, color='hotpink')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee9ee43",
   "metadata": {},
   "source": [
    "Finally, let's overlay a simple linear regression line on this plot. To do this, we'll use NumPy's `polyfit()` function to determine the best fitting regression line (represented as an intercept and slope) and then we'll draw that line on the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 100)\n",
    "y = x * 2 + np.random.normal(0, 0.2, 100)\n",
    "\n",
    "β, α = np.polyfit(x, y, 1) # fit regression line - slope and intercept\n",
    "y_predicted = α + β * x # y-values predicted by the regression line\n",
    "\n",
    "plt.xlabel('Magical X numbers')\n",
    "plt.ylabel('Special Y numbers')\n",
    "plt.scatter(x, y, color='hotpink')\n",
    "_ = plt.plot(x, y_predicted, color='black', linewidth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c761febb",
   "metadata": {},
   "source": [
    "Try playing around with the colors and styles. Need some color inspiration? [Check this page for all the standard color names you can use.](https://www.w3schools.com/cssref/css_colors.asp) Can you change the circular points into squares or triangles? Do some research to find out how it's done.\n",
    "\n",
    "A great place to start whenever you have some new data is to plot a histogram to get an overall sense of the distribution of the data points. So it's worth taking a quick moment to see how it's done in Matplotlib. As you'll see, it's super easy, so you have no excuses for not plotting histograms of your data! First, I'll generate 1000 random numbers that are normally distributed with a mean of 0 and a standard deviation of 1, and then I'll use `plt.hist()` to make the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37492ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.random.normal(0, 1, 1000)\n",
    "\n",
    "_ = plt.hist(values, color='indianred', bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8e91e",
   "metadata": {},
   "source": [
    "The `bins` argument allows you to control how many \"bins\" the datapoints are categorized into, which can give you a more granular view.\n",
    "\n",
    "Okay, now that we've played around with some a fake data, I hope you've got the general idea of how things work. Basically, you pass your data into one of the plotting functions – like `plt.plot()`, `plt.scatter()`, or `plt.hist()` – and you set some colors and labels, and hey presto, you have a plot! Of course, there are **a lot** more options for further customization. To see lots more example plots, and the code used to generate them, check the Matplotlib gallery here: https://matplotlib.org/stable/gallery/index\n",
    "\n",
    "Let's go back to the dataset from the previous section and try making some plots of real data. First, I just want to look at the results from the production test type, so let's isolate those trials first into a new data frame which we'll assign to the variable `production_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5615c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "production_df = df.query('test_type==\"production\"')\n",
    "production_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42424f6e",
   "metadata": {},
   "source": [
    "Next, I want to calculate accuracy for each of the category systems; to do this, we'll use the data frame's `.groupby()` method to group the data by category system and then calculate the mean of the `correct` column for each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c83a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_by_condition = production_df.groupby('category_system')['correct'].mean()\n",
    "accuracy_by_condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c9272",
   "metadata": {},
   "source": [
    "Right away we see that accuracy is highest in the `angle` condition and lowest in the `both` condition. Let's make a bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort in descending order\n",
    "accuracy_by_condition = accuracy_by_condition.sort_values(ascending=False)\n",
    "\n",
    "plt.ylim(0, 1) # make the y-axis go from 0 to 1\n",
    "plt.xlabel('Category system')\n",
    "plt.ylabel('Accuracy')\n",
    "_ = plt.bar(accuracy_by_condition.index, accuracy_by_condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7864f2",
   "metadata": {},
   "source": [
    "As everyone knows, bar plots are terrible! You should probably never use them. Let's do better by making a violin plot, which will show us not only the central tendency of the three conditions, but also their distributions. To make a violin plot, we need to organize the data a little bit so that we have subject-level accuracy scores for each of the category systems. To do that I will further subset the `production_df` into a separate data frame for each system, and then I'll compute subject-level accuracy using `.groupby()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd3649",
   "metadata": {},
   "outputs": [],
   "source": [
    "angl_df = production_df.query('category_system==\"angle\"')\n",
    "size_df = production_df.query('category_system==\"size\"')\n",
    "both_df = production_df.query('category_system==\"both\"')\n",
    "\n",
    "angl_accuracy_by_subject = angl_df.groupby('subject')['correct'].mean()\n",
    "size_accuracy_by_subject = size_df.groupby('subject')['correct'].mean()\n",
    "both_accuracy_by_subject = both_df.groupby('subject')['correct'].mean()\n",
    "\n",
    "data = [angl_accuracy_by_subject, size_accuracy_by_subject, both_accuracy_by_subject]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52363508",
   "metadata": {},
   "source": [
    "Print out the varibles if you're unsure what's happening. Finally, let's make the violin plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylim(0, 1) # make the y-axis go from 0 to 1\n",
    "plt.violinplot(data, showmeans=True, showextrema=False)\n",
    "plt.xlabel('Category system')\n",
    "plt.ylabel('Accuracy')\n",
    "_ = plt.xticks([1, 2, 3], labels=['Angle', 'Size', 'Both'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e64378",
   "metadata": {},
   "source": [
    "Great! Now we can get a sense of how much variation there is across participants within each condition. This is much more informative than just looking at the means (shown here with the dark blue lines).\n",
    "\n",
    "If you still have time... make a violin plot of the comprehension test data, which we ignored earlier on by extracting only the production test.\n",
    "\n",
    "Matplotlib is quite old and crusty, and it can sometimes be hard to achieve exactly the look you want. For a more modern plotting package, I'd also recommend checking out [Seaborn](https://seaborn.pydata.org). Seaborn builds on top of Matplotlib, but it's generally a bit cleaner and easier to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275373af",
   "metadata": {},
   "source": [
    "##  Arrivederci to Notebooks!\n",
    "\n",
    "This is the last class in which we'll use Jupyter Notebook. Does that make you feel happy or sad? Do you like the notebook format? Have you had any issues with it? Have you used something like this before in another language?\n",
    "\n",
    "Personally, I don't *love* notebooks, although I understand why many people do. The really nice thing is that you can mix text and code together. This makes notebooks very useful in an educational context – like this class – because we can mix code with explanations. Notebooks can also be useful in a scientific context because they allow you to clearly document your thought processes as you explore some data.\n",
    "\n",
    "However, in my experience, the notebook style of coding tends to get a bit messy and confusing, mostly because it's difficult to keep track of the current \"state\" of the underlying interpreter. For example, let's say we run the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_special_number = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290df6eb",
   "metadata": {},
   "source": [
    "Then, maybe we do some calculations with this variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = my_special_number ** 2 + 100\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65135cec",
   "metadata": {},
   "source": [
    "Okay, great! The answer is 149. Now I run some more code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db28765",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "magical_constant = 64\n",
    "my_special_number = 10\n",
    "answer = magical_constant * my_special_number\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b92de",
   "metadata": {},
   "source": [
    "Okay, 640, sure. Now go back to the previous code block and run it again – the one where we got the answer 149. Did you get the same answer? You should get 200 instead – it no longer gives you 149. Why? Because in the subsequent code block you redefined `my_special_number` to `10`, perhaps without even realizing.\n",
    "\n",
    "It's very easy to get into confusing situations like this because code blocks can be run in any order. If you lose track of what order you ran things in, you can quickly get in a pickle! Notebooks go against the top-to-bottom sequential flow that we normally expect when programming. Instead, the sequential flow – the order in which the lines of code are run – exists only in your head and remains undocumented. I don't want to totally dissuade you from using notebooks (there are many good reasons to use them), but it's worth thinking about these issues if you plan to use them in the future.\n",
    "\n",
    "If you want to think more about these problems, check out this video titled \"I Don't Like Notebooks\": https://www.youtube.com/watch?v=7jiPeIFXb6U (warning: the guy talks really fast – turn down the playback speed if you can't understand him!) I tend to agree with many of his points. What do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b046a",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "Take a dataset that you're currently working on, and try to use some the tools we've learned today. Or if you don't have your own dataset, look around online for something that interests you (e.g. climate change, covid numbers, or sports statistics). Can you import the data as a Pandas data frame? Practice extracting different rows and columns. Can you make some pretty plots?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
